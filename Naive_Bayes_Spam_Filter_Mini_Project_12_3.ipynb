{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Spam Filter\n",
    "\n",
    "Use the Scikit-Learn Naive Bayes classifiers to explore how well they perform on the UCI ML Repository spam dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import sklearn\n",
    "import urllib.request\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.      0.64    0.64    0.      0.32    0.      0.      0.      0.\n",
      "   0.      0.      0.64    0.      0.      0.      0.32    0.      1.29\n",
      "   1.93    0.      0.96    0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.778   0.      0.\n",
      "   3.756  61.    278.      1.   ]\n"
     ]
    }
   ],
   "source": [
    "# create the url for the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "\n",
    "# import the data from the UCI archive website\n",
    "raw_data = urllib.request.urlopen(url)\n",
    "dataset = np.loadtxt(raw_data, delimiter=',')\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into features and target\n",
    "X = dataset[:, 0:48]\n",
    "y = dataset[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0.0 Count: 2788 Percent: 61.0\n",
      "Label: 1.0 Count: 1813 Percent: 39.0\n"
     ]
    }
   ],
   "source": [
    "# view count of target values\n",
    "value_counts = np.unique(y, return_counts=True)\n",
    "not_spam_count = value_counts[1][0]\n",
    "spam_count = value_counts[1][1]\n",
    "total_count = not_spam_count + spam_count\n",
    "\n",
    "print(f'Label: {value_counts[0][0]} Count: {not_spam_count} Percent: {round(not_spam_count / total_count, 2)*100}')\n",
    "print(f'Label: {value_counts[0][1]} Count: {spam_count} Percent: {round(spam_count / total_count, 2)*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This shows there is a significant imbalance in data between spam and non-spam\n",
    "If there is a significant number of falsely labeled non-spam from the test data, the data may need random resampling of the spam data to improve the number of spam data seen by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB(binarize=True)\n",
      "0.8577633007600435 \n",
      "\n",
      "[[491  56]\n",
      " [ 75 299]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.90      0.88       547\n",
      "         1.0       0.84      0.80      0.82       374\n",
      "\n",
      "    accuracy                           0.86       921\n",
      "   macro avg       0.85      0.85      0.85       921\n",
      "weighted avg       0.86      0.86      0.86       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a Bernoulli Naive Bayes model\n",
    "BernNB = BernoulliNB(binarize=True)\n",
    "BernNB.fit(X_train, y_train)\n",
    "print(BernNB)\n",
    "\n",
    "y_true = y_test\n",
    "y_pred = BernNB.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_true, y_pred), '\\n')\n",
    "print(confusion_matrix(y_true, y_pred),'\\n')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "0.8816503800217155 \n",
      "\n",
      "[[454  93]\n",
      " [ 16 358]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.83      0.89       547\n",
      "         1.0       0.79      0.96      0.87       374\n",
      "\n",
      "    accuracy                           0.88       921\n",
      "   macro avg       0.88      0.89      0.88       921\n",
      "weighted avg       0.90      0.88      0.88       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MultiNB = MultinomialNB()\n",
    "MultiNB.fit(X_train, y_train)\n",
    "print(MultiNB)\n",
    "\n",
    "y_pred = MultiNB.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_true, y_pred), '\\n')\n",
    "print(confusion_matrix(y_true, y_pred),'\\n')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "0.8197611292073833 \n",
      "\n",
      "[[388 159]\n",
      " [  7 367]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.71      0.82       547\n",
      "         1.0       0.70      0.98      0.82       374\n",
      "\n",
      "    accuracy                           0.82       921\n",
      "   macro avg       0.84      0.85      0.82       921\n",
      "weighted avg       0.87      0.82      0.82       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GaussNB = GaussianNB()\n",
    "GaussNB.fit(X_train, y_train)\n",
    "print(GaussNB)\n",
    "\n",
    "y_pred = GaussNB.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_true, y_pred), '\\n')\n",
    "print(confusion_matrix(y_true, y_pred),'\\n')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Bernoulli NB with different binarize values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB(binarize=0.1)\n",
      "0.9109663409337676 \n",
      "\n",
      "[[515  32]\n",
      " [ 50 324]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.94      0.93       547\n",
      "         1.0       0.91      0.87      0.89       374\n",
      "\n",
      "    accuracy                           0.91       921\n",
      "   macro avg       0.91      0.90      0.91       921\n",
      "weighted avg       0.91      0.91      0.91       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try Bernoulli with binarize since the features are not binary\n",
    "BernNB = BernoulliNB(binarize=0.1)\n",
    "BernNB.fit(X_train, y_train)\n",
    "print(BernNB)\n",
    "\n",
    "y_pred = BernNB.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_true, y_pred), '\\n')\n",
    "print(confusion_matrix(y_true, y_pred),'\\n')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB(binarize=0.05)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.93      0.92       547\n",
      "         1.0       0.89      0.85      0.87       374\n",
      "\n",
      "    accuracy                           0.90       921\n",
      "   macro avg       0.90      0.89      0.89       921\n",
      "weighted avg       0.90      0.90      0.90       921\n",
      "\n",
      "BernoulliNB(binarize=0.75)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.89      0.89       547\n",
      "         1.0       0.84      0.85      0.84       374\n",
      "\n",
      "    accuracy                           0.87       921\n",
      "   macro avg       0.87      0.87      0.87       921\n",
      "weighted avg       0.87      0.87      0.87       921\n",
      "\n",
      "BernoulliNB(binarize=0.1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.94      0.93       547\n",
      "         1.0       0.91      0.87      0.89       374\n",
      "\n",
      "    accuracy                           0.91       921\n",
      "   macro avg       0.91      0.90      0.91       921\n",
      "weighted avg       0.91      0.91      0.91       921\n",
      "\n",
      "BernoulliNB(binarize=0.125)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.94      0.92       547\n",
      "         1.0       0.91      0.86      0.88       374\n",
      "\n",
      "    accuracy                           0.91       921\n",
      "   macro avg       0.91      0.90      0.90       921\n",
      "weighted avg       0.91      0.91      0.91       921\n",
      "\n",
      "BernoulliNB(binarize=0.15)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.94      0.92       547\n",
      "         1.0       0.91      0.86      0.88       374\n",
      "\n",
      "    accuracy                           0.91       921\n",
      "   macro avg       0.91      0.90      0.90       921\n",
      "weighted avg       0.91      0.91      0.91       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# grid search through different binarize values\n",
    "bin_values = [0.05, 0.75, 0.1, 0.125, 0.15]\n",
    "\n",
    "for bn in bin_values:\n",
    "    BernNB = BernoulliNB(binarize=bn)\n",
    "    BernNB.fit(X_train, y_train)\n",
    "    \n",
    "    print(BernNB)\n",
    "    \n",
    "    y_true = y_test\n",
    "    y_pred = BernNB.predict(X_test)\n",
    "    \n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis\n",
    "\n",
    "### Main Objective: Not have many non-spam emails end up in the spam folder \n",
    "\n",
    "Because of the nature of trade-offs in ML classification, it is important to understand what is the most important factor to optimize. In this classification problem of spam detection, **the most important thing is to optimize around Recall** because that number indicates the number of non-spam emails that were incorrectly classified as spam. This can cause issues for the user, since if an important email ends up in their spam folder they may have a bigger problem than an occasional spam email slipping through the filter into their inbox.\n",
    "\n",
    "Optimizing for Non-Spam Recall:\n",
    "\n",
    "**BernoulliNB (binarize=0.1) has the best performance**\n",
    "\n",
    "- Non-Spam Recall: 0.94\n",
    "- Non-Spam Precision: 0.91\n",
    "\n",
    "\n",
    "### If eliminating spam was the most important objective\n",
    "\n",
    "If for some reason, eliminating spam at the expense of falsely identifying non-spam emails as spam was the most important outcome. Then the **Gaussian NB model** would be the best performer, since it has the highest Spam Recall and highest Non-Spam Precision. This means that it has the best performance of correctly classifying spam emails, but at the expense of mis-classifying non-spam emails at a higher proportion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
