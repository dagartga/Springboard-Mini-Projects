{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# To create deep learning models\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, Embedding, Reshape, Dot, Concatenate, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "\n",
    "from threading import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, filename, split, cols):\n",
    "        dataframe = pd.read_csv(filename)\n",
    "        i_split = int(len(dataframe) * split)\n",
    "        self.data_train = dataframe.get(cols).values[:i_split]\n",
    "        self.data_test = dataframe.get(cols).values[i_split:]\n",
    "        self.len_train = len(self.data_train)\n",
    "        self.len_test = len(self.data_test)\n",
    "        self.len_train_windows = None\n",
    "        \n",
    "    def get_train_data(self, seq_len, normalise):\n",
    "        data_x = []\n",
    "        data_y = []\n",
    "        for i in range(self.len_train - seq_len):\n",
    "            x, y = self._next_window(i, seq_len, normalise)\n",
    "            data_x.append(x)\n",
    "            data_y.append(y)\n",
    "        return np.array(data_x), np.array(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a neural net model\n",
    "\n",
    "class Model():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = Sequential()\n",
    "        \n",
    "    def build_model(self, configs):\n",
    "        timer = Timer()\n",
    "        timer.start()\n",
    "        \n",
    "\n",
    "        for layer in configs['model']['layers']:\n",
    "            neurons = layer['neurons'] if 'neurons' in layer else None\n",
    "            dropout_rate = layer['rate'] if 'rate' in layer else None\n",
    "            activation = layer['activation'] if 'activation' in layer else None\n",
    "            return_seq = layer['return_seq'] if 'return_seq' in layer else None\n",
    "            input_timesteps = layer['input_timesteps'] if 'input_timesteps' in layer else None\n",
    "            input_dim = layer['input_dim'] if 'input_dim' in layer else None\n",
    "\n",
    "            if layer['type'] == 'dense':\n",
    "                self.model.add(Dense(neurons, activation=activation))\n",
    "            if layer['type'] == 'lstm':\n",
    "                self.model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=return_seq))\n",
    "            if layer['type'] == 'dropout':\n",
    "                self.model.add(Dropout(dropout_rate))\n",
    "\n",
    "        self.model.compile(loss=configs['model']['loss'], optimizer=configs['model']['optimizer'])\n",
    "        \n",
    "        print('[Model] Model Compiled')\n",
    "        Timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Sine Wave Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6368/1808270770.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# train our model using the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mconfigs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'config.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m data = DataLoader(\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config.json'"
     ]
    }
   ],
   "source": [
    "# train our model using the data\n",
    "\n",
    "configs = json.load(open('config.json', 'r'))\n",
    "\n",
    "data = DataLoader(\n",
    "    os.path.join('data', configs['data']['filename']),\n",
    "    configs['data']['train_test_split'],\n",
    "    configs['data']['columns']\n",
    "    )\n",
    "\n",
    "model = Model()\n",
    "model.build_model(configs)\n",
    "x, y = data.get_train_data(\n",
    "    seq_len = configs['data']['sequence_length'],\n",
    "    normalise = configs['data']['normalise']\n",
    "    )\n",
    "\n",
    "model.train(\n",
    "    x,\n",
    "    y,\n",
    "    epochs = configs['training']['epochs'],\n",
    "    batch_size = configs['training']['batch_size']\n",
    "    )\n",
    "\n",
    "x_test, y_test = data.get_test_data(\n",
    "    seq_len = configs['data']['sequence_length'],\n",
    "    normalise = configs['data']['normalise']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_point_by_point(self, data):\n",
    "    #Predict each timestep given the last sequence of true data, in effect only predicting 1 step ahead each time\n",
    "    predicted = self.model.predict(data)\n",
    "    predicted = np.reshape(predicted, (predicted.size,))\n",
    "    return predicted\n",
    "\n",
    "def predict_sequence_full(self, data, window_size):\n",
    "    #Shift the window by 1 new prediction each time, re-run predictions on new window\n",
    "    curr_frame = data[0]\n",
    "    predicted = []\n",
    "    for i in range(len(data)):\n",
    "        predicted.append(self.model.predict(curr_frame[newaxis,:,:])[0,0])\n",
    "        curr_frame = curr_frame[1:]\n",
    "        curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n",
    "    return predicted\n",
    "\n",
    "predictions_pointbypoint = model.predict_point_by_point(x_test)\n",
    "plot_results(predictions_pointbypoint, y_test)\n",
    "\n",
    "predictions_fullseq = model.predict_sequence_full(x_test, configs['data']['sequence_length'])\n",
    "plot_results(predictions_fullseq, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not So Simple Stock Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_windows(self, window_data, single_window=False):\n",
    "    '''Normalise window with a base value of zero'''\n",
    "    normalised_data = []\n",
    "    window_data = [window_data] if single_window else window_data\n",
    "    for window in window_data:\n",
    "        normalised_window = []\n",
    "        for col_i in range(window.shape[1]):\n",
    "            normalised_col = [((float(p) / float(window[0, col_i])) - 1) for p in window[:, col_i]]\n",
    "            normalised_window.append(normalised_col)\n",
    "                # reshape and transpose array back into original multidimensional format\n",
    "        normalised_window = np.array(normalised_window).T\t\t\t\t\n",
    "        normalised_data.append(normalised_window)\n",
    "    return np.array(normalised_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = json.load(open('config.json', 'r'))\n",
    "\n",
    "data = DataLoader(\n",
    "    os.path.join('data', configs['data']['filename']),\n",
    "    configs['data']['train_test_split'],\n",
    "    configs['data']['columns']\n",
    "    )\n",
    "\n",
    "model = Model()\n",
    "model.build_model(configs)\n",
    "x, y = data.get_train_data(\n",
    "    seq_len = configs['data']['sequence_length'],\n",
    "    normalise = configs['data']['normalise']\n",
    "    )\n",
    "\n",
    "# out-of memory generative training\n",
    "steps_per_epoch = math.ceil((data.len_train - configs['data']['sequence_length']) / configs['training']['batch_size'])\n",
    "model.train_generator(\n",
    "    data_gen = data.generate_train_batch(\n",
    "        seq_len = configs['data']['sequence_length'],\n",
    "        batch_size = configs['training']['batch_size'],\n",
    "        normalise = configs['data']['normalise']\n",
    "    ),\n",
    "    epochs = configs['training']['epochs'],\n",
    "    batch_size = configs['training']['batch_size'],\n",
    "    steps_per_epoch = steps_per_epoch\n",
    "    )\n",
    "\n",
    "x_test, y_test = data.get_test_data(\n",
    "    seq_len = configs['data']['sequence_length'],\n",
    "    normalise = configs['data']['normalise']\n",
    "    )\n",
    "\n",
    "predictions_multiseq = model.predict_sequences_multiple(x_test, configs['data']['sequence_length'], configs['data']['sequence_length'])\n",
    "predictions_fullseq = model.predict_sequence_full(x_test, configs['data']['sequence_length'])\n",
    "predictions_pointbypoint = model.predict_point_by_point(x_test)        \n",
    "\n",
    "plot_results_multiple(predictions_multiseq, y_test, configs['data']['sequence_length'])\n",
    "plot_results(predictions_fullseq, y_test)\n",
    "plot_results(predictions_pointbypoint, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
